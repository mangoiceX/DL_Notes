#### 深度学习的使用层面

##### 训练、开发、测试

开发集（cross validation set, hold out, development set) 常用dev表示。

机器学习常用的是训练集：测试集=7:3

当数据量比较少的时候（少于1万），深度学习常用的划分比例为 训练：开发： 测试 = 6:2:2

当数据量有100w时，train: dev: test = 98:1:1

dev set和test set要有一样的分布。

test set是对最终选定的神经网络做出无偏估计，可以没有。

平时只划分两个部分，实际上是train/dev,而不是train/test。

##### 偏差与方差

相对于基准误差判断。

假设前提：train和dev同分布。

高偏差high bias:模型在train和dev上表现都很差。（模型能力不足，模型有问题）

1，检查数据集的问题；

2，调整模型；（更大的模型，训练更长时间，更换网络结构）

直到过拟合训练数据。

高方差high variance:模型在train上表现很好，但是在dev上表现很差。（过拟合）

1，采用更多数据；

2，正则化；

3，调整模型；

##### 正则化

直观感受：使用正则化可以减少神经元权重，权重减少会让输入值减少，那么就会是输入值大致分布在激活函数的接近线性的区域，这会降低模型对复杂边界的拟合能力。

添加正则化之后，在绘制损失函数变化图象时，记得使用新的损失函数定义（包括正则化项的）。

##### Dropout

Intuition: Can't rely on any one feature, so have to spread out weights.

dropout将产生收缩权重的平方范数的效果。

在参数量较多的层，可以设置较低的保留概率；

一般不对输入进行dropout，最少设置0.9。

CV的输入信息很多，所以常常使用dropout。dropout主要在CV中使用，其他领域使用较少。

其他领域一开始不要使用dropout，过拟合之后再使用。

缺点：

1，为了使用交叉验证，你要搜索更多的超级参数。

2，代价函数J不再明确定义，失去调试工具。（可以先关闭dropout，确保J单调下降，然后再打开）

##### Data augmentation

##### Early stopping 

dev测试准确率一般会先下降再上升。在转折点停止，参数w的L2范数不会很大。

early stopping 使用一种方法同时解决两个优化目标：损失值下降、不过拟合，无法分开处理两个目标（吴恩达如是说）。只需要运行一次梯度下降就可以找到较小值，较大值。

L2正则化可以使超参数搜索空间容易分解和搜索，缺点是要不断尝试$\lambda$的值。



##### 归一化

对测试集和训练集采用同样的参数值进行归一化。

归一化之后可以使用更大的学习率。

##### 初始化

Xavier初始化

设置标准差：

Relu激活函数：np.randm.randn(shape) * 标准差

$\sqrt{(\frac{2}{n^{[l-1]}})}$

Tanh激活函数：

$\sqrt{(\frac{1}{n^{[l-1]}})}$

有些会使用：

$\sqrt{(\frac{2}{n^{[l-1]}+n^{[l]}})}$​

梯度检验：Gradient checking

不要在训练过程使用地图检验，它只用于调试

（使用框架自动计算梯度，所以这个在实践中基本无用）



